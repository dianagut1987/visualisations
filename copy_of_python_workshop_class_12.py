# -*- coding: utf-8 -*-
"""Copy of Python Workshop  Class 12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sawBXBFl3Gu2F2uS7K5HFfRCfp3oNKnI

#Dimensionality reduction
"""

print(__doc__)


# Code source: GaÃ«l Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets


# import some data to play with
iris = datasets.load_iris()
y = iris.target
#print(iris)
print(type(iris))
#print(iris['DESCR'])
print(type(iris['data']))

X = iris.data[:, [0,3]]  # we only take the first two features.

x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

plt.figure(2, figsize=(8, 6))
plt.clf()

# Plot the training points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,
            edgecolor='k')
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())

plt.boxplot(iris.data)

# To getter a better understanding of interaction of the dimensions
# plot the first three PCA dimensions
from sklearn.decomposition import PCA
algo=PCA(n_components=4)
algo.fit(iris.data)
print(algo)
print(algo.components_)
print(algo.explained_variance_)

X_reduced=algo.transform(iris.data)
plt.scatter(X_reduced[:,0],X_reduced[:,1],c=y,cmap=plt.cm.Set1)

algo=PCA(n_components=3)
algo.fit(iris.data)
print(algo.components_)
print(algo.explained_variance_ratio_)

X_reduced=algo.transform(iris.data)

fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,
           cmap=plt.cm.Set1, edgecolor='k', s=40)
ax.set_title("First three PCA directions")
ax.set_xlabel("1st eigenvector")
ax.w_xaxis.set_ticklabels([])
ax.set_ylabel("2nd eigenvector")
ax.w_yaxis.set_ticklabels([])
ax.set_zlabel("3rd eigenvector")
ax.w_zaxis.set_ticklabels([])

#[[ 0.36138659 -0.08452251  0.85667061  0.3582892 ]
# [ 0.65658877  0.73016143 -0.17337266 -0.07548102]]

#nice graphs
from sklearn.manifold import TSNE
X = iris.data
X_embedded = TSNE(n_components=2).fit_transform(X)
#print(X_embedded)
plt.scatter(X_embedded[:,0],X_embedded[:,1],c=y, cmap=plt.cm.Set1)

X_embedded = TSNE(n_components=2).fit_transform(X_reduced)
#print(X_embedded)
plt.scatter(X_embedded[:,0],X_embedded[:,1],c=y, cmap=plt.cm.Set1)

"""#Clustering"""

plt.scatter(X_embedded[:,0],X_embedded[:,1])

from sklearn.cluster import DBSCAN
import numpy as np
X = X_embedded
clustering = DBSCAN(eps=1, min_samples=2).fit(X)
print(clustering.labels_)
plt.scatter(X_embedded[:,0],X_embedded[:,1],c=clustering.labels_)

from sklearn.cluster import DBSCAN
import numpy as np
X = iris.data
clustering = DBSCAN(eps=0.4, min_samples=2).fit(X)
print(clustering.labels_)
plt.scatter(X_embedded[:,0],X_embedded[:,1],c=clustering.labels_)

from sklearn.cluster import KMeans
X = iris.data
clustering = KMeans(n_clusters=3).fit(X)
print(clustering.labels_)
plt.scatter(X_embedded[:,0],X_embedded[:,1],c=clustering.labels_)

#try spectral clustering

fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2],edgecolor='k', s=40)
ax.set_title("First three PCA directions")
ax.set_xlabel("1st eigenvector")
ax.w_xaxis.set_ticklabels([])
ax.set_ylabel("2nd eigenvector")
ax.w_yaxis.set_ticklabels([])
ax.set_zlabel("3rd eigenvector")
ax.w_zaxis.set_ticklabels([])

X=X_reduced
clustering = DBSCAN(eps=0.2, min_samples=2).fit(X)

fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2],c=clustering.labels_,edgecolor='k', s=40)
ax.set_title("First three PCA directions")
ax.set_xlabel("1st eigenvector")
ax.w_xaxis.set_ticklabels([])
ax.set_ylabel("2nd eigenvector")
ax.w_yaxis.set_ticklabels([])
ax.set_zlabel("3rd eigenvector")
ax.w_zaxis.set_ticklabels([])